{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph DB Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values \n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "NEO4j_URI = 'bolt://localhost:7687'\n",
    "NEO4j_USER = 'neo4j'\n",
    "NEO4j_PASSWORD = os.environ.get('pass')\n",
    "GROQ_API = os.environ.get('GROQ_API')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect with graphDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(url=NEO4j_URI, username=NEO4j_USER, password=NEO4j_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.graphs.neo4j_graph.Neo4jGraph at 0x2bbb14ad0c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Groq API for free LLM Models specifically using llama-3.2-90b-text-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(groq_api_key=GROQ_API, model_name='llama-3.2-90b-vision-preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002BBCB3C1C00>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002BBCB3C27D0>, model_name='llama-3.2-90b-vision-preview', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating appropriate dataset for GraphDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalGraph:\n",
    "    def __init__(self, graph: Neo4jGraph):\n",
    "        self.graph = graph\n",
    "        self.debug_info = {\n",
    "            'total_triggers_found': 0,\n",
    "            'triggers_per_event': []\n",
    "        }\n",
    "\n",
    "    def clear_database(self):\n",
    "        clear_queries = [\n",
    "            \"MATCH (n) DETACH DELETE n\",\n",
    "            \"CALL apoc.schema.assert({}, {})\"\n",
    "        ]\n",
    "        for query in clear_queries:\n",
    "            try:\n",
    "                self.graph.query(query)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning during cleanup: {e}\")\n",
    "    \n",
    "    def create_indexes(self):\n",
    "        constraints = [\n",
    "            \"CREATE CONSTRAINT IF NOT EXISTS FOR (e:Event) REQUIRE e.id IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Cause) REQUIRE c.id IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT IF NOT EXISTS FOR (e:Effect) REQUIRE e.id IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT IF NOT EXISTS FOR (t:Trigger) REQUIRE t.id IS UNIQUE\"\n",
    "        ]\n",
    "        for query in constraints:\n",
    "            try:\n",
    "                self.graph.query(query)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning during index creation: {e}\")\n",
    "    \n",
    "    def clean_text(self, text: str, preserve_case: bool = False) -> str:\n",
    "        if pd.isna(text) or text is None:\n",
    "            return \"\"\n",
    "        cleaned = str(text).strip()\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "        cleaned = cleaned.replace('\"', \"'\").replace('\\\\', '')\n",
    "        return cleaned if preserve_case else cleaned.lower()\n",
    "    \n",
    "    def generate_hash(self, text: str, event_id: str = \"\") -> str:\n",
    "        text_to_hash = f\"{text}_{event_id}\" if event_id else text\n",
    "        return hashlib.md5(text_to_hash.encode()).hexdigest()\n",
    "    \n",
    "    def extract_elements(self, tagged_text: str) -> Optional[Dict]:\n",
    "        if not isinstance(tagged_text, str) or tagged_text == 'NoTag':\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            patterns = {\n",
    "                'causes': r'<cause>((?:(?!</cause>).)*)</cause>',\n",
    "                'effects': r'<effect>((?:(?!</effect>).)*)</effect>',\n",
    "                'triggers': r'<trigger>((?:(?!</trigger>).)*)</trigger>'\n",
    "            }\n",
    "            \n",
    "            elements = {}\n",
    "            for key, pattern in patterns.items():\n",
    "                matches = re.findall(pattern, tagged_text, re.DOTALL | re.IGNORECASE)\n",
    "                if key == 'triggers':\n",
    "                    elements[key] = [m.strip() for m in matches if m.strip()]\n",
    "                    self.debug_info['triggers_per_event'].append(len(elements[key]))\n",
    "                    self.debug_info['total_triggers_found'] += len(elements[key])\n",
    "                else:\n",
    "                    cleaned_matches = [m.strip() for m in matches if m.strip()]\n",
    "                    elements[key] = list(dict.fromkeys(cleaned_matches))\n",
    "            \n",
    "            return elements if any(elements.values()) else None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting elements: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def create_event_graph(self, text: str, tagged_text: str, event_id: str):\n",
    "        elements = self.extract_elements(tagged_text)\n",
    "        if not elements:\n",
    "            return\n",
    "        \n",
    "        event_query = \"\"\"\n",
    "        MERGE (e:Event {id: $id})\n",
    "        SET e.text = $text,\n",
    "            e.tagged_text = $tagged_text,\n",
    "            e.created_at = datetime()\n",
    "        \"\"\"\n",
    "        \n",
    "        self.graph.query(\n",
    "            event_query,\n",
    "            params={\n",
    "                'id': event_id,\n",
    "                'text': self.clean_text(text),\n",
    "                'tagged_text': tagged_text\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        for cause in elements['causes']:\n",
    "            cause_id = self.generate_hash(self.clean_text(cause))\n",
    "            self.graph.query(\"\"\"\n",
    "            MATCH (e:Event {id: $event_id})\n",
    "            MERGE (c:Cause {id: $cause_id})\n",
    "            SET c.text = $cause_text\n",
    "            MERGE (c)-[r:CAUSES]->(e)\n",
    "            SET r.created_at = datetime()\n",
    "            \"\"\", params={'event_id': event_id, 'cause_id': cause_id, 'cause_text': cause})\n",
    "        \n",
    "        for effect in elements['effects']:\n",
    "            effect_id = self.generate_hash(self.clean_text(effect))\n",
    "            self.graph.query(\"\"\"\n",
    "            MATCH (e:Event {id: $event_id})\n",
    "            MERGE (eff:Effect {id: $effect_id})\n",
    "            SET eff.text = $effect_text\n",
    "            MERGE (e)-[r:RESULTS_IN]->(eff)\n",
    "            SET r.created_at = datetime()\n",
    "            \"\"\", params={'event_id': event_id, 'effect_id': effect_id, 'effect_text': effect})\n",
    "        \n",
    "        for trigger in elements['triggers']:\n",
    "            trigger_id = self.generate_hash(self.clean_text(trigger), event_id)\n",
    "            self.graph.query(\"\"\"\n",
    "            MATCH (e:Event {id: $event_id})\n",
    "            MERGE (t:Trigger {id: $trigger_id})\n",
    "            SET t.text = $trigger_text,\n",
    "                t.event_id = $event_id\n",
    "            MERGE (e)-[r:HAS_TRIGGER]->(t)\n",
    "            SET r.created_at = datetime()\n",
    "            \"\"\", params={\n",
    "                'event_id': event_id,\n",
    "                'trigger_id': trigger_id,\n",
    "                'trigger_text': trigger\n",
    "            })\n",
    "    \n",
    "    def analyze_dataset(self, csv_path: str):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df = df.replace({np.nan: None})\n",
    "        tagged_rows = df[df['tagged_sentence'] != 'NoTag']\n",
    "        \n",
    "        total_stats = {'causes': 0, 'effects': 0, 'triggers': 0}\n",
    "        unique_elements = {'causes': set(), 'effects': set(), 'triggers': set()}\n",
    "        \n",
    "        print(\"Analyzing dataset...\")\n",
    "        for _, row in tagged_rows.iterrows():\n",
    "            elements = self.extract_elements(str(row['tagged_sentence']))\n",
    "            if elements:\n",
    "                for key in ['causes', 'effects', 'triggers']:\n",
    "                    total_stats[key] += len(elements[key])\n",
    "                    unique_elements[key].update(elements[key])\n",
    "        \n",
    "        print(\"\\nDataset Analysis:\")\n",
    "        print(f\"Total tagged sentences: {len(tagged_rows)}\")\n",
    "        print(\"\\nTotal elements found:\")\n",
    "        for key, value in total_stats.items():\n",
    "            print(f\"Total {key}: {value}\")\n",
    "        print(\"\\nUnique elements:\")\n",
    "        for key, value in unique_elements.items():\n",
    "            print(f\"Unique {key}: {len(value)}\")\n",
    "        \n",
    "        print(\"\\nTrigger Statistics:\")\n",
    "        print(f\"Total triggers found: {self.debug_info['total_triggers_found']}\")\n",
    "        trigger_counts = Counter(self.debug_info['triggers_per_event'])\n",
    "        print(\"Events by trigger count:\")\n",
    "        for count, freq in sorted(trigger_counts.items()):\n",
    "            print(f\"{count} trigger(s): {freq} events\")\n",
    "    \n",
    "    def load_dataset(self, csv_path: str, clear_existing: bool = True):\n",
    "        if clear_existing:\n",
    "            self.clear_database()\n",
    "        self.create_indexes()\n",
    "        \n",
    "        self.analyze_dataset(csv_path)\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        df = df.replace({np.nan: None})\n",
    "        tagged_rows = df[df['tagged_sentence'] != 'NoTag']\n",
    "        \n",
    "        print(\"\\nLoading data into graph...\")\n",
    "        for idx, row in tqdm(tagged_rows.iterrows(), total=len(tagged_rows)):\n",
    "            try:\n",
    "                self.create_event_graph(\n",
    "                    text=str(row['text']),\n",
    "                    tagged_text=str(row['tagged_sentence']),\n",
    "                    event_id=f\"event_{idx}\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing row {idx}: {str(e)}\")\n",
    "        \n",
    "        stats = self.get_graph_statistics()\n",
    "        print(\"\\nFinal Graph Statistics:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "    def get_graph_statistics(self) -> Dict:\n",
    "        node_stats_query = \"\"\"\n",
    "        MATCH (n)\n",
    "        RETURN {\n",
    "            events: count(CASE WHEN n:Event THEN 1 END),\n",
    "            causes: count(CASE WHEN n:Cause THEN 1 END),\n",
    "            effects: count(CASE WHEN n:Effect THEN 1 END),\n",
    "            triggers: count(CASE WHEN n:Trigger THEN 1 END)\n",
    "        } as stats\n",
    "        \"\"\"\n",
    "        \n",
    "        rel_stats_query = \"\"\"\n",
    "        MATCH ()-[r]->()\n",
    "        RETURN count(r) as relationships\n",
    "        \"\"\"\n",
    "        \n",
    "        node_results = self.graph.query(node_stats_query)\n",
    "        rel_results = self.graph.query(rel_stats_query)\n",
    "        \n",
    "        stats = node_results[0]['stats']\n",
    "        stats['relationships'] = rel_results[0]['relationships']\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing dataset...\n",
      "\n",
      "Dataset Analysis:\n",
      "Total tagged sentences: 1030\n",
      "\n",
      "Total elements found:\n",
      "Total causes: 1177\n",
      "Total effects: 1125\n",
      "Total triggers: 1106\n",
      "\n",
      "Unique elements:\n",
      "Unique causes: 1153\n",
      "Unique effects: 1119\n",
      "Unique triggers: 577\n",
      "\n",
      "Trigger Statistics:\n",
      "Total triggers found: 1106\n",
      "Events by trigger count:\n",
      "0 trigger(s): 100 events\n",
      "1 trigger(s): 806 events\n",
      "2 trigger(s): 95 events\n",
      "3 trigger(s): 15 events\n",
      "4 trigger(s): 8 events\n",
      "5 trigger(s): 3 events\n",
      "6 trigger(s): 3 events\n",
      "\n",
      "Loading data into graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:21<00:00, 48.26it/s]\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\n        MATCH (n)\\n        RETURN {\\n            events: count(CASE WHEN n:Event THEN 1 END),\\n            causes: count(CASE WHEN n:Cause THEN 1 END),\\n            effects: count(CASE WHEN n:Effect THEN 1 END),\\n            triggers: count(CASE WHEN n:Trigger THEN 1 END)\\n        } as stats\\n        '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Graph Statistics:\n",
      "events: 1021\n",
      "effects: 1118\n",
      "triggers: 1102\n",
      "causes: 1147\n",
      "relationships: 3404\n"
     ]
    }
   ],
   "source": [
    "graph = Neo4jGraph(url=NEO4j_URI, username=NEO4j_USER, password=NEO4j_PASSWORD)\n",
    "graph_creation = CausalGraph(graph)\n",
    "graph_creation.load_dataset('Causal_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if correct data has been added to DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_distribution(csv_path: str):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    total_sentences = len(df)\n",
    "    tagged_sentences = df[df['tagged_sentence'] != 'NoTag'].shape[0]\n",
    "    \n",
    "    cause_count = df['tagged_sentence'].str.count('<cause>').sum()\n",
    "    effect_count = df['tagged_sentence'].str.count('<effect>').sum()\n",
    "    trigger_count = df['tagged_sentence'].str.count('<trigger>').sum()\n",
    "    \n",
    "    print(f\"Dataset Analysis:\")\n",
    "    print(f\"Total sentences: {total_sentences}\")\n",
    "    print(f\"Tagged sentences: {tagged_sentences}\")\n",
    "    print(f\"Unique causes: {cause_count}\")\n",
    "    print(f\"Unique effects: {effect_count}\")\n",
    "    print(f\"Unique triggers: {trigger_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Analysis:\n",
      "Total sentences: 2005\n",
      "Tagged sentences: 1030\n",
      "Unique causes: 1182.0\n",
      "Unique effects: 1132.0\n",
      "Unique triggers: 1107.0\n"
     ]
    }
   ],
   "source": [
    "analyze_dataset_distribution('Causal_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
